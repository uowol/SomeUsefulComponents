{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader     # 데이터로더는 데이터셋을 iterable하게 감싸는 역할\n",
    "from torchvision import datasets            # 데이터셋은 샘플과 정답을 저장함\n",
    "from torchvision.transforms import ToTensor\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image_title = 'A horse in the space.png'\n",
    "\n",
    "prompts = [\n",
    "    \"A horse in the space\",\n",
    "    \"A dog in the space\",\n",
    "    \"A bear in the space\",\n",
    "    \"A person in the space\",\n",
    "    \"A horse in the park\",\n",
    "    \"A dog in the park\",\n",
    "    \"A bear in the park\",\n",
    "    \"A person in the park\",\n",
    "]\n",
    "\n",
    "image = preprocess(Image.open(image_title)).unsqueeze(0).to(device)\n",
    "text = clip.tokenize(prompts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "img_embeddings = [torch.zeros([1, 512]) for i in range(model.visual.transformer.layers)]\n",
    "\n",
    "#%% model.encode_image\n",
    "\n",
    "# image -> tokens\n",
    "x = model.visual.conv1(image.type(model.visual.conv1.weight.dtype))\n",
    "x = x.reshape(x.shape[0], x.shape[1], -1)   # shape = [*, width, grid ** 2]\n",
    "x = x.permute(0, 2, 1)                      # shape = [*, grid ** 2, width]\n",
    "x = torch.cat([model.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "x = x + model.visual.positional_embedding.to(x.dtype)\n",
    "x = model.visual.ln_pre(x)\n",
    "\n",
    "# tokens -> transformer -> feature_embeddings\n",
    "x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "\n",
    "for i in range(model.visual.transformer.layers):\n",
    "    x = model.visual.transformer.resblocks[i](x)\n",
    "    tmp = x.permute(1, 0, 2)\n",
    "    tmp = model.visual.ln_post(tmp[:, 0, :])\n",
    "    if model.visual.proj is not None:\n",
    "        tmp = tmp @ model.visual.proj\n",
    "    img_embeddings[i].copy_(tmp)\n",
    "\n",
    "x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "print(x.shape)  \n",
    "\n",
    "x = model.visual.ln_post(x[:, 0, :])    # [CLS] token의 임베딩을 사용\n",
    "print(x.shape)  \n",
    "\n",
    "if model.visual.proj is not None:\n",
    "    x = x @ model.visual.proj\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 77, 512])\n",
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "txt_embeddings = [torch.zeros([8, 512]) for i in range(model.transformer.layers)]\n",
    "\n",
    "#%% model.encode_image\n",
    "\n",
    "x = model.token_embedding(text).type(model.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "x = x + model.positional_embedding.type(model.dtype)\n",
    "x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "\n",
    "for i in range(model.transformer.layers):\n",
    "    x = model.transformer.resblocks[i](x)\n",
    "    tmp = x.permute(1, 0, 2)\n",
    "    tmp = model.ln_final(tmp).type(model.dtype)\n",
    "    tmp = tmp[torch.arange(tmp.shape[0]), text.argmax(dim=-1)] @ model.text_projection\n",
    "    txt_embeddings[i].copy_(tmp)\n",
    "\n",
    "\n",
    "x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "x = model.ln_final(x).type(model.dtype)\n",
    "print(x.shape)\n",
    "\n",
    "x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ model.text_projection    \n",
    "    # x.shape[0] : 들어온 단어 토큰의 개수\n",
    "    # x[[(which_word_token)], (which_end_token)] : [CLS] token이 아니라 end token의 임베딩을 사용\n",
    "    # text.argmax(dim=-1) : end token의 위치\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_embeddings[0][[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_similarity_by_layer = [[(img_embeddings[i] @ txt_embeddings[j][[0]].T).item() for j in range(12)] for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.460453033447266, 10.72868076297972, 86)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score\n",
    "argmax = np.argmax(img_similarity_by_layer)\n",
    "\n",
    "score1 = np.max(img_similarity_by_layer)\n",
    "score2 = np.mean(img_similarity_by_layer)\n",
    "\n",
    "threshold = 10\n",
    "score3 = sum(list(map(lambda x: sum([y>threshold for y in x]), img_similarity_by_layer)))\n",
    "\n",
    "score1, score2, score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:\tA horse in the space\n",
      "score1:\t33.826358795166016\n",
      "score2:\t12.221461418602201\n",
      "score3:\t14\n",
      "====================\n",
      "label:\tA dog in the space\n",
      "score1:\t26.979454040527344\n",
      "score2:\t12.280097925000721\n",
      "score3:\t13\n",
      "====================\n",
      "label:\tA bear in the space\n",
      "score1:\t26.947742462158203\n",
      "score2:\t12.698545568519169\n",
      "score3:\t13\n",
      "====================\n",
      "label:\tA person in the space\n",
      "score1:\t30.46843719482422\n",
      "score2:\t14.02111851506763\n",
      "score3:\t13\n",
      "====================\n",
      "label:\tA horse in the park\n",
      "score1:\t25.63037872314453\n",
      "score2:\t11.307973388168547\n",
      "score3:\t6\n",
      "====================\n",
      "label:\tA dog in the park\n",
      "score1:\t20.79569435119629\n",
      "score2:\t10.3964384496212\n",
      "score3:\t1\n",
      "====================\n",
      "label:\tA bear in the park\n",
      "score1:\t22.559795379638672\n",
      "score2:\t11.075195771124628\n",
      "score3:\t6\n",
      "====================\n",
      "label:\tA person in the park\n",
      "score1:\t23.460453033447266\n",
      "score2:\t10.72868076297972\n",
      "score3:\t11\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "threshold = 20\n",
    "\n",
    "for _class in range(8):\n",
    "    img_similarity_by_layer = [[(img_embeddings[i] @ txt_embeddings[j][[_class]].T).item() for j in range(12)] for i in range(12)]\n",
    "    score1 = np.max(img_similarity_by_layer)\n",
    "    score2 = np.mean(img_similarity_by_layer)\n",
    "    score3 = sum(list(map(lambda x: sum([y>threshold for y in x]), img_similarity_by_layer)))\n",
    "    print(f\"label:\\t{prompts[_class]}\\nscore1:\\t{score1}\\nscore2:\\t{score2}\\nscore3:\\t{score3}\\n{'='*20}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn    # Neural Network Layer ? \n",
    "from torch.utils.data import DataLoader, Dataset    # 데이터로더는 데이터셋을 iterable하게 감싸는 역할\n",
    "from torchvision import datasets                    # 데이터셋은 샘플과 정답을 저장함\n",
    "from torchvision.transforms import ToTensor         # 데이터셋을 텐서로 바꾸는듯?\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flickr dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import io, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\n",
    "from transformers import VisionEncoderDecoderModel , ViTFeatureExtractor\n",
    "from transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "class config : \n",
    "    ENCODER = \"google/vit-base-patch16-224\"\n",
    "    DECODER = \"gpt2\"\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VAL_BATCH_SIZE = 8\n",
    "    VAL_EPOCHS = 1\n",
    "    LR = 5e-5\n",
    "    SEED = 42\n",
    "    MAX_LEN = 128\n",
    "    SUMMARY_LEN = 20\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MEAN = (0.485, 0.456, 0.406)\n",
    "    STD = (0.229, 0.224, 0.225)\n",
    "    TRAIN_PCT = 0.95\n",
    "    NUM_WORKERS = mp.cpu_count()\n",
    "    EPOCHS = 3\n",
    "    IMG_SIZE = (224,224)\n",
    "    LABEL_MASK = -100\n",
    "    TOP_K = 1000\n",
    "    TOP_P = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(config.ENCODER)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.DECODER)\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(config.IMG_SIZE), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=0.5, \n",
    "            std=0.5\n",
    "        )\n",
    "   ]\n",
    ")\n",
    "df=  pd.read_csv(\"Flickr8k/captions.txt\")\n",
    "train_df , val_df = train_test_split(df , test_size = 0.2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, df,root_dir,tokenizer,feature_extractor, transform = None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer= tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = 50\n",
    "    def __len__(self,):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.df.caption.iloc[idx]\n",
    "        image = self.df.image.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir , image)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img= self.transform(img)\n",
    "        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values\n",
    "        captions = self.tokenizer(caption,\n",
    "                                 padding='max_length',\n",
    "                                 max_length=self.max_length).input_ids\n",
    "        captions = [caption if caption != self.tokenizer.pad_token_id else -100 for caption in captions]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(captions)}\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImgDataset(train_df, root_dir = \"Flickr8k/Images\",\n",
    "                            tokenizer=tokenizer,feature_extractor = feature_extractor ,transform = transforms)\n",
    "val_dataset = ImgDataset(val_df , root_dir = \"Flickr8k/Images\",\n",
    "                            tokenizer=tokenizer,feature_extractor = feature_extractor , transform  = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-0.5373, -0.5373, -0.5373,  ..., -0.4275, -0.4431, -0.4431],\n",
       "          [-0.5059, -0.5216, -0.5059,  ..., -0.4118, -0.4431, -0.4431],\n",
       "          [-0.5059, -0.5059, -0.5059,  ..., -0.4118, -0.4275, -0.4275],\n",
       "          ...,\n",
       "          [ 0.6000,  0.3333,  0.0667,  ...,  0.7098,  0.7255,  0.7255],\n",
       "          [ 0.3804,  0.4588,  0.6157,  ...,  0.7255,  0.7569,  0.7569],\n",
       "          [ 0.6784,  0.4902,  0.2078,  ...,  0.7098,  0.6627,  0.6471]],\n",
       " \n",
       "         [[ 0.3098,  0.3098,  0.3098,  ...,  0.3098,  0.3255,  0.3255],\n",
       "          [ 0.3255,  0.3098,  0.3255,  ...,  0.3412,  0.3255,  0.3255],\n",
       "          [ 0.3255,  0.3255,  0.3255,  ...,  0.3569,  0.3412,  0.3412],\n",
       "          ...,\n",
       "          [ 0.4745,  0.1922, -0.0824,  ...,  0.5529,  0.5843,  0.5843],\n",
       "          [ 0.2706,  0.3333,  0.5216,  ...,  0.5686,  0.6157,  0.6157],\n",
       "          [ 0.5686,  0.3804,  0.1608,  ...,  0.5529,  0.5216,  0.5059]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9686,  0.9686],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9686,  0.9686],\n",
       "          [ 0.9843,  0.9843,  0.9843,  ...,  0.9843,  0.9686,  0.9686],\n",
       "          ...,\n",
       "          [ 0.4275,  0.1608, -0.0980,  ...,  0.5216,  0.5059,  0.5059],\n",
       "          [ 0.1922,  0.3020,  0.5059,  ...,  0.5373,  0.5373,  0.5373],\n",
       "          [ 0.4431,  0.3176,  0.0824,  ...,  0.5059,  0.4431,  0.4275]]]),\n",
       " 'labels': tensor([  32, 3290, 2491,  319,  262, 6450,  764, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_dataloader:\n",
    "    print(x, val_dataset[0][x].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: pixel_values\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [53], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m test_dataloader:    \u001b[39m# X = data, y = label\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of X [N, C, H, W]: \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)    \u001b[39m# N = batch size, C = Color, H, W\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of y: \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "batch_size = 64 \n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:    # X = data, y = label\n",
    "    print(f\"Shape of X [N, C, H, W]: {X}\")    # N = batch size, C = Color, H, W\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "img_arr = []\n",
    "caption_arr = []\n",
    "for idx, (imgs, captions) in enumerate(loader):\n",
    "    for idx in range(4):\n",
    "        img_arr.append(imgs[idx])\n",
    "        caption_arr.append(captions[idx])\n",
    "            \n",
    "        plt.title(caption_arr[idx])\n",
    "        plt.imshow(img_arr[idx].permute(1,2,0))\n",
    "            \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e13486f50ad8491adee3900c65648710623e910042c80717865a7ec3732c806"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
